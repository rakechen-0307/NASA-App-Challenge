{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to JS-Pytorch's documentation","text":"<p>For access to the source code, visit The GitHub repo.</p>"},{"location":"#about","title":"About","text":"<ul> <li>JS-PyTorch is a Deep Learning JavaScript library built from scratch, to closely follow PyTorch's syntax.</li> <li>This means that you can use this library to train, test and deploy Neural Networks, with node.js or on a web browser.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>This is a node package, and can be installed with  npm (Node Package Manager). It has full sopport of node 20.15.1, which is the latest LTS (Long-Term Support) node version. </p> <p>In most operating systems, it should also work for more recent versions.</p>"},{"location":"#macos","title":"MacOS","text":"<ul> <li>First, install node with the command line, as described on the node website:</li> </ul> <pre><code># installs nvm (Node Version Manager)\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\n# download and install Node.js (you may need to restart the terminal)\nnvm install 20\n# verifies the right Node.js version is in the environment\nnode -v # should print `v20.15.1`\n# verifies the right npm version is in the environment\nnpm -v # should print `10.7.0`\n</code></pre> <ul> <li>Now, use npm to install Js-PyTorch locally:</li> </ul> <pre><code># installs js-pytorch\nnpm install js-pytorch\n# if needed, install older version of js-pytorch\nnvm install js-pytorch@0.1.0\n</code></pre> <ul> <li>Finally, require the package in your javascript file:</li> </ul> <pre><code>const { torch } = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\n</code></pre>"},{"location":"#linux","title":"Linux","text":"<ul> <li>First, install node with the command line, as described on the node website:</li> </ul> <pre><code># installs nvm (Node Version Manager)\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash\n# download and install Node.js (you may need to restart the terminal)\nnvm install 20\n# verifies the right Node.js version is in the environment\nnode -v # should print `v20.15.1`\n# verifies the right npm version is in the environment\nnpm -v # should print `10.7.0`\n</code></pre> <ul> <li>Now, use npm to install Js-PyTorch locally:</li> </ul> <pre><code># installs js-pytorch\nnpm install js-pytorch\n# if needed, install older version of js-pytorch\nnvm install js-pytorch@0.1.0\n</code></pre> <ul> <li>Finally, require the package in your javascript file:</li> </ul> <pre><code>const { torch } = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\n</code></pre>"},{"location":"#windows","title":"Windows","text":"<ul> <li> <p>First, download node from the prebuilt installer on the node website:</p> </li> <li> <p>Now, use npm to install Js-PyTorch locally:</p> </li> </ul> <pre><code># installs js-pytorch\nnpm install js-pytorch\n# if needed, install older version of js-pytorch\nnvm install js-pytorch@0.1.0\n</code></pre> <p>Note:If this throws an error, you might need to install the latest version of Visual Studio, including the \"Desktop development with C++\" workload.</p> <ul> <li>Finally, require the package in your javascript file:</li> </ul> <pre><code>const { torch } = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>If you have detected a bug on the library, please file a Bug Report using a GitHub issue, and feel free to reach out to me on my LinkedIn or email.</li> <li>If you would like to see a new feature in Js-PyTorch, file a New Feature issue.</li> <li>Finally, if you would like to contribute, create a merge request to the <code>develop</code> branch. I will try to answer as soon as possible. All help is really appreciated! Here is a list of the developer tools:<ul> <li>Build for Distribution by running <code>npm run build</code>. CJS and ESM modules and <code>index.d.ts</code> will be output in the <code>dist/</code> folder.</li> <li>Check the Code with ESLint at any time, running <code>npm run lint</code>.</li> <li>Run tests run <code>npm test</code>.</li> <li>Improve Code Formatting with prettier, running <code>npm run prettier</code>.</li> <li>Performance Benchmarks are also included in the <code>tests/benchmarks/</code> directory. Run all benchmarks with <code>npm run bench</code> and save new benchmarks with <code>npm run bench:update</code>.</li> </ul> </li> </ul>"},{"location":"layers/","title":"Layers","text":"<p>In this section are listed all of the Layers and Modules.</p>"},{"location":"layers/#nnlinear","title":"nn.Linear","text":"<pre><code>new nn.Linear(in_size,\n              out_size,\n              device, \n              bias, \n              xavier)\n</code></pre> <p>Applies a linear transformation to the input tensor. Input is matrix-multiplied by a <code>w</code> tensor and added to a <code>b</code> tensor.</p> <p>Parameters</p> <ul> <li>in_size (number) - Size of the last dimension of the input data.</li> <li>out_size (number) - Size of the last dimension of the output data.</li> <li>device (string) - Device on which the model's calculations will run. Either <code>'cpu'</code> or <code>'gpu'</code>.</li> <li>bias (boolean) - Whether to use a bias term <code>b</code>.</li> <li>xavier (boolean) - Whether to use Xavier initialization on the weights.</li> </ul> <p>Learnable Variables</p> <ul> <li>w - [input_size, output_size] Tensor.</li> <li>b - [output_size] Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const linear = new nn.Linear(10,15,'gpu');\n&gt;&gt;&gt; let x = torch.randn([100,50,10], true, 'gpu');\n&gt;&gt;&gt; let y = linear.forward(x);\n&gt;&gt;&gt; y.shape\n// [100, 50, 15]\n</code></pre> <p></p>"},{"location":"layers/#nnmultiheadselfattention","title":"nn.MultiHeadSelfAttention","text":"<pre><code>new nn.MultiHeadSelfAttention(in_size,\n                              out_size,\n                              n_heads,\n                              n_timesteps,\n                              dropout_prob,\n                              device)\n</code></pre> <p>Applies a self-attention layer on the input tensor.</p> <ul> <li>Matrix-multiplies input by <code>Wk</code>, <code>Wq</code>, <code>Wv</code>, resulting in Key, Query and Value tensors.</li> <li>Computes attention multiplying Query and transpose Key.</li> <li>Applies Mask, Dropout and Softmax to attention activations.</li> <li>Multiplies result by Values.</li> <li>Multiplies result by <code>residual_proj</code>.</li> <li>Applies final Dropout.</li> </ul> <p>Parameters</p> <ul> <li>in_size (number) - Size of the last dimension of the input data.</li> <li>out_size (number) - Size of the last dimension of the output data.</li> <li>n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads.</li> <li>n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer.</li> <li>dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization).</li> <li>device (string) - Device on which the model's calculations will run. Either <code>'cpu'</code> or <code>'gpu'</code>.</li> </ul> <p>Learnable Variables</p> <ul> <li>Wk - [input_size, input_size] Tensor.</li> <li>Wq - [input_size, input_size] Tensor.</li> <li>Wv - [input_size, input_size] Tensor.</li> <li>residual_proj - [input_size, output_size] Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const att = new nn.MultiHeadSelfAttention(10, 15, 2, 32, 0.2, 'gpu');\n&gt;&gt;&gt; let x = torch.randn([100,50,10], true, 'gpu');\n&gt;&gt;&gt; let y = att.forward(x);\n&gt;&gt;&gt; y.shape\n// [100, 50, 15]\n</code></pre> <p></p>"},{"location":"layers/#nnfullyconnected","title":"nn.FullyConnected","text":"<pre><code>new nn.FullyConnected(in_size,\n                      out_size,\n                      dropout_prob,\n                      device,\n                      bias)\n</code></pre> <p>Applies a fully-connected layer on the input tensor.</p> <ul> <li>Matrix-multiplies input by Linear layer <code>l1</code>, upscaling the input.</li> <li>Passes tensor through ReLU.</li> <li>Matrix-multiplies tensor by Linear layer <code>l2</code>, downscaling the input.</li> <li>Passes tensor through Dropout.</li> </ul> <pre><code>forward(x: Tensor): Tensor {\n    let z = this.l1.forward(x);\n    z = this.relu.forward(z);\n    z = this.l2.forward(z);\n    z = this.dropout.forward(z);\n    return z;\n}\n</code></pre> <p>Parameters</p> <ul> <li>in_size (number) - Size of the last dimension of the input data.</li> <li>out_size (number) - Size of the last dimension of the output data.</li> <li>dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization).</li> <li>device (string) - Device on which the model's calculations will run. Either <code>'cpu'</code> or <code>'gpu'</code>.</li> <li>bias (boolean) - Whether to use a bias term <code>b</code>.</li> </ul> <p>Learnable Variables</p> <ul> <li>l1 - [input_size, 4input_size] Tensor.</li> <li>l2 - [4input_size, input_size] Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const fc = new nn.FullyConnected(10, 15, 0.2, 'gpu');\n&gt;&gt;&gt; let x = torch.randn([100,50,10], true, 'gpu');\n&gt;&gt;&gt; let y = fc.forward(x);\n&gt;&gt;&gt; y.shape\n// [100, 50, 15]\n</code></pre> <p></p>"},{"location":"layers/#nnblock","title":"nn.Block","text":"<pre><code>new nn.Block(in_size,\n             out_size,\n             n_heads,\n             n_timesteps,\n             dropout_prob,\n             device)\n</code></pre> <p>Applies a transformer Block layer on the input tensor.</p> <pre><code>forward(x: Tensor): Tensor {\n    // Pass through Layer Norm and Self Attention:\n    let z = x.add(this.att.forward(this.ln1.forward(x)));\n    // Pass through Layer Norm and Fully Connected:\n    z = z.add(this.fcc.forward(this.ln2.forward(z)));\n    return z;\n}\n</code></pre> <p>Parameters</p> <ul> <li>in_size (number) - Size of the last dimension of the input data.</li> <li>out_size (number) - Size of the last dimension of the output data.</li> <li>n_heads (boolean) - Number of parallel attention heads the data is divided into. In_size must be divided evenly by n_heads.</li> <li>n_timesteps (boolean) - Number of timesteps computed in parallel by the transformer.</li> <li>dropout_prob (boolean) - probability of randomly dropping an activation during training (to improve regularization).</li> <li>device (string) - Device on which the model's calculations will run. Either <code>'cpu'</code> or <code>'gpu'</code>.</li> </ul> <p>Learnable Modules</p> <ul> <li>nn.MultiHeadSelfAttention - <code>Wk</code>, <code>Wq</code>, <code>Wv</code>, <code>residual_proj</code>.</li> <li>nn.LayerNorm - <code>gamma</code>, <code>beta</code>.</li> <li>nn.FullyConnecyed - <code>l1</code>, <code>l2</code>.</li> <li>nn.LayerNorm - <code>gamma</code>, <code>beta</code>.</li> </ul> <p></p>"},{"location":"layers/#nnembedding","title":"nn.Embedding","text":"<pre><code>new nn.Embedding(in_size,\n                 embed_size)\n</code></pre> <p>Embedding table, with a number of embeddings equal to the vocabulary size of the model <code>in_size</code>, and size of each embedding equal to <code>embed_size</code>. For each element in the input tensor (integer), returns the embedding indexed by the integer.</p> <pre><code>forward(idx: Tensor): Tensor {\n   // Get embeddings indexed by input (idx):\n   let x = this.E.at(idx);\n   return x;\n}\n</code></pre> <p>Parameters</p> <ul> <li>in_size (number) - Number of different classes the model can predict (vocabulary size).</li> <li>embed_size (number) - Dimension of each embedding generated.</li> </ul> <p>Learnable Parameters</p> <ul> <li>E - [vocab_size, embed_size] Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const batch_size = 32;\n&gt;&gt;&gt; const number_of_timesteps = 256;\n&gt;&gt;&gt; const embed = new nn.Embedding(10, 64);\n&gt;&gt;&gt; let x = torch.randint(0, 10, [batch_size, number_of_timesteps]);\n&gt;&gt;&gt; let y = embed.forward(x);\n&gt;&gt;&gt; y.shape\n// [32, 256, 64]\n</code></pre> <p></p>"},{"location":"layers/#nnpositionalembedding","title":"nn.PositionalEmbedding","text":"<pre><code>new nn.PositionalEmbedding(input_size,\n                           embed_size)\n</code></pre> <p>Embedding table, with a number of embeddings equal to the input size of the model <code>input_size</code>, and size of each embedding equal to <code>embed_size</code>. For each element in the input tensor, returns the embedding indexed by it's position.</p> <pre><code>forward(idx: Tensor): Tensor {\n   // Get dimension of the input:\n   const [B, T] = idx.shape;\n   // Gets positional embeddings for each element along \"T\": (Batch, Timesteps) =&gt; (Batch, Timesteps, Embed)\n   const x = this.E.at([...Array(T).keys()]);\n   return x\n}\n</code></pre> <p>Parameters</p> <ul> <li>input_size (number) - Number of different embeddings in the lookup table (size of the input).</li> <li>embed_size (number) - Dimension of each embedding generated.</li> </ul> <p>Learnable Parameters</p> <ul> <li>E - [input_size, embed_size] Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const batch_size = 32;\n&gt;&gt;&gt; const number_of_timesteps = 256;\n&gt;&gt;&gt; const embed = new nn.PositionalEmbedding(number_of_timesteps, 64);\n&gt;&gt;&gt; let x = torch.randint(0, 10, [batch_size, number_of_timesteps]);\n&gt;&gt;&gt; let y = embed.forward(x);\n&gt;&gt;&gt; y.shape\n// [32, 256, 64]\n</code></pre> <p></p>"},{"location":"layers/#nnrelu","title":"nn.ReLU","text":"<pre><code>new nn.ReLU()\n</code></pre> <p>Rectified Linear Unit activation function. This implementation is leaky for stability. For each element in the incoming tensor:</p> <ul> <li>If element is positive, no change.</li> <li>If element is negative, multiply by 0.001.</li> </ul> <p>Parameters</p> <ul> <li>None</li> </ul> <p>Learnable Parameters</p> <ul> <li>None</li> </ul> <p></p>"},{"location":"layers/#nnsoftmax","title":"nn.Softmax","text":"<pre><code>new nn.Softmax()\n</code></pre> <p>Softmax activation function. Rescales the data in the input tensor, along the <code>dim</code> dimension. The sum of every element along this dimension is one, and every element is between zero and one.</p> <pre><code>forward(x: Tensor, dim = -1): Tensor {\n   z = exp(z);\n   const out = z.div(z.sum(dim, true));\n   return out;\n   return x\n}\n</code></pre> <p>Parameters</p> <ul> <li>None</li> </ul> <p>Learnable Parameters</p> <ul> <li>None</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const softmax = new nn.Softmax();\n&gt;&gt;&gt; let x = torch.randn([2,4]);\n&gt;&gt;&gt; let y = softmax.forward(x, -1);\n&gt;&gt;&gt; y.data\n// [[0.1, 0.2, 0.8, 0.0],\n//  [0.6, 0.1, 0.2, 0.1]]\n</code></pre> <p></p>"},{"location":"layers/#nndropout","title":"nn.Dropout","text":"<pre><code>new nn.Dropout(drop_prob: number)\n</code></pre> <p>Dropout class. For each element in input tensor, has a <code>drop_prob</code> chance of setting it to zero.</p> <p>Parameters</p> <ul> <li>drop_prob (number) - Probability to drop each value in input, from 0 to 1.</li> </ul> <p>Learnable Parameters</p> <ul> <li>None</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const dropout = new nn.Dropout(0.5);\n&gt;&gt;&gt; let x = torch.ones([2,4]);\n&gt;&gt;&gt; let y = dropout.forward(x);\n&gt;&gt;&gt; y.data\n// [[1, 0, 0, 1],\n//  [0, 1, 0, 1]]\n</code></pre> <p></p>"},{"location":"layers/#nnlayernorm","title":"nn.LayerNorm","text":"<pre><code>new nn.LayerNorm(n_embed: number)\n</code></pre> <p>LayerNorm class. Normalizes the data, with a mean of 0 and standard deviation of 1, across the last dimension. This is done as described in the LayerNorm paper.</p> <p>Parameters</p> <ul> <li>n_embed (number) - Size of the last dimension of the input.</li> </ul> <p>Learnable Parameters</p> <ul> <li>gamma (number) - Constant to multiply output by (initialized as 1).</li> <li>beta (number) - Constant to add to output (initialized as 0).</li> </ul> <p></p>"},{"location":"layers/#nncrossentropyloss","title":"nn.CrossEntropyLoss","text":"<pre><code>new nn.CrossEntropyLoss()\n</code></pre> <p>Cross Entropy Loss function. Computes the cross entropy loss between the target and the input tensor.</p> <ul> <li>First, calculates softmax of input tensor.</li> <li>Then, selects the elements of the input corresponding to the correct class in the target.</li> <li>Gets the negative log of these elements.</li> <li>Adds all of them, and divides by the number of elemets.</li> </ul> <p>Parameters</p> <ul> <li>None</li> </ul> <p>Learnable Parameters</p> <ul> <li>None</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; const number_of_classes = 10;\n&gt;&gt;&gt; const input_size = 64;\n&gt;&gt;&gt; const loss_func = new nn.CrossEntropyLoss();\n&gt;&gt;&gt; let x = torch.randn([input_size, number_of_classes]);\n&gt;&gt;&gt; let y = torch.randint(0, number_of_classes, [input_size]);\n&gt;&gt;&gt; let loss = loss_func.forward(x, y);\n&gt;&gt;&gt; loss.data\n// 2.3091357\n</code></pre>"},{"location":"operations/","title":"Tensor Operations","text":"<p>In this section are listed all of the Tensor Operation methods.</p>"},{"location":"operations/#torchadd","title":"torch.add","text":"<pre><code>torch.add(a,\n          b) \u2192 Tensor\n</code></pre> <ul> <li>If both tensors are scalars, the simple sum is returned.</li> <li>If one tensor is a scalar, the element-wise sum of the scalar and the tensor is returned.</li> <li>If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and add them element-wise.</li> </ul> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor or number.</li> <li>b (Tensor | number) - Other Tensor or number.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,1,2,3],\n                          [6,7,8,9]]);\n&gt;&gt;&gt; let b = torch.tensor([1]);\n&gt;&gt;&gt; let c = torch.add(a,b);\n&gt;&gt;&gt; c.data;\n//[[2,2,3,4],\n// [7,8,9,10]]\n&gt;&gt;&gt; b = torch.tensor([[0],\n                      [2]]);\n&gt;&gt;&gt; c = torch.add(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,1,2,3],\n// [8,9,10,11]]\n&gt;&gt;&gt; b = torch.tensor([[0,0,0,0],\n                      [0,0,100,0]]);\n&gt;&gt;&gt; c = torch.add(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,1,2,3],\n// [6,7,108,9]]\n</code></pre> <p>Note: <code>torch.add(a, b)</code> is the same as <code>a.add(b)</code>.</p> <p></p>"},{"location":"operations/#torchsub","title":"torch.sub","text":"<pre><code>torch.sub(a,\n          b) \u2192 Tensor\n</code></pre> <ul> <li>If both tensors are scalars, the simple subtraction is returned.</li> <li>If one tensor is a scalar, the element-wise subtraction of the scalar and the tensor is returned.</li> <li>If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and subtract them element-wise.</li> </ul> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor or number.</li> <li>b (Tensor | number) - Other Tensor or number.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,1,2,3],\n                          [6,7,8,9]]);\n&gt;&gt;&gt; let b = torch.tensor([1]);\n&gt;&gt;&gt; let c = torch.sub(a,b);\n&gt;&gt;&gt; c.data;\n//[[0,0,1,2],\n// [5,6,7,8]]\n&gt;&gt;&gt; b = torch.tensor([[0],\n                      [2]]);\n&gt;&gt;&gt; c = torch.sub(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,1,2,3],\n// [4,5,6,7]]\n&gt;&gt;&gt; b = torch.tensor([[0,0,0,0],\n                      [0,0,8,0]]);\n&gt;&gt;&gt; c = torch.sub(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,1,2,3],\n// [6,7,0,9]]\n</code></pre> <p>Note: <code>torch.sub(a, b)</code> is the same as <code>a.sub(b)</code>.</p> <p></p>"},{"location":"operations/#torchneg","title":"torch.neg","text":"<pre><code>torch.neg(a) \u2192 Tensor\n</code></pre> <p>Returns the element-wise opposite of the given Tensor.</p> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor or number.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([1]);\n&gt;&gt;&gt; let b = torch.neg(a);\n&gt;&gt;&gt; c.data;\n// [-1]\n&gt;&gt;&gt; a = torch.tensor([-3]);\n&gt;&gt;&gt; b = torch.neg(a);\n&gt;&gt;&gt; c.data;\n// [3]\n&gt;&gt;&gt; a = torch.tensor([[0,1,0,-1],\n                      [-3,2,1,0]]);\n&gt;&gt;&gt; b = torch.neg(a);\n&gt;&gt;&gt; c.data;\n//[[0,-1,0,1],\n// [3,-2,-1,0]]\n</code></pre> <p>Note: <code>torch.neg(a)</code> is the same as <code>a.neg()</code>.</p> <p></p>"},{"location":"operations/#torchmul","title":"torch.mul","text":"<pre><code>torch.mul(a,\n          b) \u2192 Tensor\n</code></pre> <ul> <li>If both tensors are scalars, the simple dot product is returned.</li> <li>If one tensor is a scalar, the element-wise product of the scalar and the tensor is returned.</li> <li>If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and multiply them element-wise.</li> </ul> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor or number.</li> <li>b (Tensor | number) - Other Tensor or number.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,1,2,3],\n                          [6,7,8,9]]);\n&gt;&gt;&gt; let b = torch.tensor([2]);\n&gt;&gt;&gt; let c = torch.mul(a,b);\n&gt;&gt;&gt; c.data;\n//[[0,0,1,2],\n// [5,6,7,8]]\n&gt;&gt;&gt; b = torch.tensor([[0],\n                      [-1]]);\n&gt;&gt;&gt; c = torch.mul(a,b);\n&gt;&gt;&gt; c.data;\n//[[0, 0, 0, 0],\n// [-6,-7,-8,-9]]\n&gt;&gt;&gt; b = torch.tensor([[0,0,0,0],\n                      [0,0,8,0]]);\n&gt;&gt;&gt; c = torch.mul(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,1,2,3],\n// [6,7,0,9]]\n</code></pre> <p>Note: <code>torch.mul(a, b)</code> is the same as <code>a.mul(b)</code>.</p> <p></p>"},{"location":"operations/#torchdiv","title":"torch.div","text":"<pre><code>torch.div(a,\n          b) \u2192 Tensor\n</code></pre> <ul> <li>If both tensors are scalars, the simple division is returned.</li> <li>If one tensor is a scalar, the element-wise division of the scalar and the tensor is returned.</li> <li>If tensors both are n-dimensional tensors, JS-PyTorch will attempt to broadcast their shapes, and divide them element-wise.</li> </ul> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor or number.</li> <li>b (Tensor | number) - Other Tensor or number.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[2,-2,4,6],\n                          [6,-6,8,8]]);\n&gt;&gt;&gt; let b = torch.tensor([2]);\n&gt;&gt;&gt; let c = torch.div(a,b);\n&gt;&gt;&gt; c.data;\n//[[1,-1,2,3],\n// [3,-3,4,4]]\n&gt;&gt;&gt; b = torch.tensor([[1],\n                      [-1]]);\n&gt;&gt;&gt; c = torch.div(a,b);\n&gt;&gt;&gt; c.data;\n//[[2,-2, 4, 6],\n// [-6,6,-8,-8]]\n&gt;&gt;&gt; b = torch.tensor([[1,1,1,1],\n                      [1,1,16,1]]);\n&gt;&gt;&gt; c = torch.div(a,b);\n&gt;&gt;&gt; c.data;\n//[[2,-2, 4, 6],\n// [6,-6,0.5,8]]\n</code></pre> <p>Note: <code>torch.div(a, b)</code> is the same as <code>a.div(b)</code>.</p> <p></p>"},{"location":"operations/#torchmatmul","title":"torch.matmul","text":"<pre><code>torch.matlul(a,\n             b) \u2192 Tensor\n</code></pre> <p>Performs matrix multiplication between the last two dimensions of each Tensor. If inputs are of shape <code>[H,W]</code> and <code>[W,C]</code>, the output will have shape <code>[H,C]</code>. If the two Tensors have more than two dimensions, they can be broadcast:</p> <ul> <li><code>[B,N,H,W], [B,N,W,C] =&gt; [B,N,H,C]</code></li> <li><code>[B,N,H,W], [W,C] =&gt; [B,N,H,C]</code></li> <li><code>[H,W], [B,N,W,C] =&gt; [B,N,H,C]</code></li> <li><code>[B,N,H,W], [1,1,W,C] =&gt; [B,N,H,C]</code></li> </ul> <p>Parameters</p> <ul> <li>a (Tensor | number) - Input Tensor.</li> <li>b (Tensor | number) - Other Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,1,1,2], \n                          [3,1,0,0]]); // Shape [2,4]\n&gt;&gt;&gt; let b = torch.tensor([[1],\n                          [0],\n                          [0],\n                          [0]]); // Shape [4,1]\n&gt;&gt;&gt; let c = torch.matmul(a,b); // Shape [2,1]\n&gt;&gt;&gt; c.data;\n//[[1],\n// [3]]\n</code></pre> <p>Note: <code>torch.matmul(a, b)</code> is the same as <code>a.matmul(b)</code>.</p> <p></p>"},{"location":"operations/#torchsum","title":"torch.sum","text":"<pre><code>torch.sum(a,\n          dim,\n          keepdims=false) \u2192 Tensor\n</code></pre> <p>Gets the sum of the Tensor over a specified dimension.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>dim (integer) - Dimension to perform the sum over.</li> <li>keepdims (boolean) - Whether to keep dimensions of original tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.ones([4,3], false, 'gpu');\n&gt;&gt;&gt; a.data;\n//[[1, 1, 1],\n// [1, 1, 1]\n// [1, 1, 1]\n// [1, 1, 1]]\n&gt;&gt;&gt; let b = torch.sum(a, 0);\n&gt;&gt;&gt; b.data;\n// [[4, 4, 4]]\n&gt;&gt;&gt; b = torch.sum(a, 1);\n&gt;&gt;&gt; b.data;\n// [[3],\n//  [3],\n//  [3],\n//  [3]]\n&gt;&gt;&gt; b = torch.sum(a, 0, true);\n&gt;&gt;&gt; b.data;\n//[[4, 4, 4],\n// [4, 4, 4]\n// [4, 4, 4]\n// [4, 4, 4]]\n</code></pre> <p>Note: <code>torch.sum(a)</code> is the same as <code>a.sum()</code>.</p> <p></p>"},{"location":"operations/#torchmean","title":"torch.mean","text":"<pre><code>torch.mean(a,\n          dim,\n          keepdims=false) \u2192 Tensor\n</code></pre> <p>Gets the mean of the Tensor over a specified dimension.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>dim (integer) - Dimension to get the mean of.</li> <li>keepdims (boolean) - Whether to keep dimensions of original tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randint(0, 2, [2,3], false, 'gpu');\n&gt;&gt;&gt; a.data;\n//[[0, 1, 0],\n// [1, 1, 1]]\n&gt;&gt;&gt; let b = torch.mean(a, 0);\n&gt;&gt;&gt; b.data;\n// [[0.5, 1, 0.5]]\n&gt;&gt;&gt; b = torch.mean(a, 1);\n&gt;&gt;&gt; b.data;\n// [[0.333333],\n//  [1]]\n&gt;&gt;&gt; b = torch.mean(a, 0, true);\n&gt;&gt;&gt; b.data;\n//[[0.5, 1, 0.5],\n// [0.5, 1, 0.5]]\n</code></pre> <p>Note: <code>torch.mean(a)</code> is the same as <code>a.mean()</code>.</p> <p></p>"},{"location":"operations/#torchvariance","title":"torch.variance","text":"<pre><code>torch.variance(a,\n          dim,\n          keepdims=false) \u2192 Tensor\n</code></pre> <p>Gets the variance of the Tensor over a specified dimension.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>dim (integer) - Dimension to get the variance of.</li> <li>keepdims (boolean) - Whether to keep dimensions of original tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randint(0, 3, [3,2], false, 'gpu');\n&gt;&gt;&gt; a.data;\n//[[0, 2],\n// [2, 1],\n// [0, 1]]\n&gt;&gt;&gt; let b = torch.variance(a, 0);\n&gt;&gt;&gt; b.data;\n// [[0.9428, 0.471404]]\n&gt;&gt;&gt; b = torch.variance(a, 0, true);\n&gt;&gt;&gt; b.data;\n//[[0.9428, 0.471404]\n// [0.9428, 0.471404]\n// [0.9428, 0.471404]]\n</code></pre> <p>Note: <code>torch.variance(a)</code> is the same as <code>a.variance()</code>.</p> <p></p>"},{"location":"operations/#torchtranspose","title":"torch.transpose","text":"<pre><code>torch.transpose(a,\n          dim1,\n          dim2) \u2192 Tensor\n</code></pre> <p>Transposes the tensor along two consecutive dimensions.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>dim1 (integer) - First dimension.</li> <li>dim2 (boolean) - Second dimension.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randint(0, 3, [3,2], false, 'gpu');\n&gt;&gt;&gt; a.data;\n//[[0, 2],\n// [2, 1],\n// [0, 1]]\n&gt;&gt;&gt; let b = torch.transpose(a, -1, -2);\n&gt;&gt;&gt; b.data;\n//[[0, 2, 0],\n// [2, 1, 1]]\n</code></pre> <p>Note: <code>torch.transpose(a)</code> is the same as <code>a.transpose()</code>.</p> <p></p>"},{"location":"operations/#torchat","title":"torch.at","text":"<pre><code>torch.at(a,\n          dim1,\n          dim2) \u2192 Tensor\n</code></pre> <p>If a single Array <code>index1</code> is passed, returns the elements in the tensor indexed by this Array: <code>tensor[index1]</code>. If a two Arrays <code>index1</code> and <code>index2</code> are passed, returns the elements in the tensor indexed by <code>tensor[index1][index2]</code>.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>index1 (Array) - Array containing indexes to extract data from in first dimension.</li> <li>index2 (Array) - Array containing indexes to extract data from in second dimension.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,1,2,3],\n                [6,7,8,9]]);\n&gt;&gt; let b = torch.at(a,[0,1,1], [2,0,3]);\n&gt;&gt;&gt; b.data;\n// [2,6,9]\n&gt;&gt;&gt; b = torch.at(a,[0,1,0]);\n&gt;&gt;&gt; b.data;\n// [[1,1,2,3],\n//  [6,7,8,9],\n//  [1,1,2,3]])\n</code></pre> <p>Note: <code>torch.at(a)</code> is the same as <code>a.at()</code>.</p> <p></p>"},{"location":"operations/#torchmasked_fill","title":"torch.masked_fill","text":"<pre><code>torch.masked_fill(a,\n                  condition,\n                  value) \u2192 Tensor\n</code></pre> <p>A condition function scans the <code>a</code> tensor element-wise, returning <code>true</code> or <code>false</code>. In places within the <code>a</code> tensor where the \"condition\" function returns True, we set the value to <code>value</code>.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>condition (function) - Function that returns True or False element-wise.</li> <li>value (number) - Value to fill Tensor when condition is met.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,5,2,3],\n                   [6,7,2,9]]);\n&gt;&gt;&gt; let b = torch.masked_fill(a, mask, (el) =&gt; {return el &gt; 3}, 0);\n&gt;&gt;&gt; b.data;\n// [[1,0,2,3],\n//  [0,0,2,0]]\n</code></pre> <p>Note: <code>torch.masked_fill(a)</code> is the same as <code>a.masked_fill()</code>.</p> <p></p>"},{"location":"operations/#torchpow","title":"torch.pow","text":"<pre><code>torch.pow(a,\n          n) \u2192 Tensor\n</code></pre> <p>Returns tensor to element-wise power of n.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> <li>n (function) - Exponent.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,-5],\n                          [6,7]]);\n&gt;&gt;&gt; let b = torch.pow(a, 2);\n&gt;&gt;&gt; b.data;\n// [[1,25],\n//  [36,49]]\n</code></pre> <p>Note: <code>torch.pow(a)</code> is the same as <code>a.pow()</code>.</p> <p></p>"},{"location":"operations/#torchsqrt","title":"torch.sqrt","text":"<pre><code>torch.sqrt(a) \u2192 Tensor\n</code></pre> <p>Returns element-wise square root of the tensor.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,9],\n                          [4,16]]);\n&gt;&gt;&gt; let b = torch.sqrt(a);\n&gt;&gt;&gt; b.data;\n// [[1,3],\n//  [2,4]]\n</code></pre> <p>Note: <code>torch.sqrt(a)</code> is the same as <code>a.sqrt()</code>.</p> <p></p>"},{"location":"operations/#torchexp","title":"torch.exp","text":"<pre><code>torch.exp(a) \u2192 Tensor\n</code></pre> <p>Returns element-wise exponentiation of the tensor.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,2],\n                          [0,-1]]);\n&gt;&gt;&gt; let b = torch.exp(a);\n&gt;&gt;&gt; b.data;\n// [[2.71828,7.389056],\n//  [1.00000,0.36788]]\n</code></pre> <p>Note: <code>torch.exp(a)</code> is the same as <code>a.exp()</code>.</p> <p></p>"},{"location":"operations/#torchlog","title":"torch.log","text":"<pre><code>torch.log(a) \u2192 Tensor\n</code></pre> <p>Returns element-wise natural log of the tensor.</p> <p>Parameters</p> <ul> <li>a (Tensor) - Input Tensor.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,2],\n                          [0.01,3]]);\n&gt;&gt;&gt; let b = torch.log(a);\n&gt;&gt;&gt; b.data;\n// [[0.00000,0.693147],\n//  [-4.6051,1.098612]]\n</code></pre> <p>Note: <code>torch.log(a)</code> is the same as <code>a.log()</code>.</p>"},{"location":"tensor/","title":"Tensor","text":"<p>The Tensor is the main object of this library. In this section are listed all of the Tensor methods.</p>"},{"location":"tensor/#torchtensor","title":"torch.tensor","text":"<pre><code>torch.tensor(data,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Returns a tensor filled with the data in the argument <code>data</code>.</p> <p>Parameters</p> <ul> <li>data (Array) - Javascript Array containing the data to be stored in the Tensor. This array can have any number of dimensions, but must have a homogenous shape, and only numbers.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tensor([[1,2,3],[4,5,6]], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[1,2,3],\n// [4,5,6]]\n</code></pre> <p></p>"},{"location":"tensor/#torchzeros","title":"torch.zeros","text":"<pre><code>torch.zeros(*shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Returns a tensor filled with zeros with dimensions like <code>shape</code>.</p> <p>Parameters</p> <ul> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.zeros([3,2], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[0, 0],\n// [0, 0]\n// [0, 0]]\n</code></pre> <p></p>"},{"location":"tensor/#torchones","title":"torch.ones","text":"<pre><code>torch.ones(*shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Returns a tensor filled with ones with dimensions like <code>shape</code>.</p> <p>Parameters</p> <ul> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.ones([3,2], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[1, 1],\n// [1, 1]\n// [1, 1]]\n</code></pre> <p></p>"},{"location":"tensor/#torchtril","title":"torch.tril","text":"<pre><code>torch.tril(*shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Returns a 2D lower triangular tensor.</p> <p>Parameters</p> <ul> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.tril([4,3], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[1, 0, 0],\n// [1, 1, 0]\n// [1, 1, 1]\n// [1, 1, 1]]\n&gt;&gt;&gt; let b = torch.tril([3,4], false, 'gpu');\n&gt;&gt;&gt; console.log(b.data);\n//[[1, 0, 0, 0],\n// [1, 1, 0, 0]\n// [1, 1, 1, 0]\n</code></pre> <p></p>"},{"location":"tensor/#torchrandn","title":"torch.randn","text":"<pre><code>torch.randn(*shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Returns a tensor filled with randomly sampled data with dimensions like <code>shape</code>. The sample is from a normal distribution.</p> <p>Parameters</p> <ul> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[1.001, 0.122],\n// [-0.93, 0.125]\n// [0.123,-0.001]]\n</code></pre> <p></p>"},{"location":"tensor/#torchrand","title":"torch.rand","text":"<pre><code>torch.rand(*shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Creates new instance of the Tensor class filled with numbers in a uniform distribution in ]0,1[.</p> <p>Parameters</p> <ul> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.rand([3,2], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[0.011, 0.122],\n// [-0.03, 0.105]\n// [-0.90,-0.202]]\n</code></pre> <p></p>"},{"location":"tensor/#torchrandint","title":"torch.randint","text":"<pre><code>torch.rand(low,\n            high,\n            *shape,\n            requires_grad=false,\n            device='cpu') \u2192 Tensor\n</code></pre> <p>Creates new instance of the Tensor class filled with random integers between <code>low</code> and <code>high</code>.</p> <p>Parameters</p> <ul> <li>low - Lowest integer that can be sampled.</li> <li>high - One above highest integer that can be sampled.</li> <li>shape (Array) - Javascript Array containing the shape of the Tensor.</li> <li>requires_grad (boolean) - Whether to keep track of this tensor's gradients. Set this to true if you want to learn this parameter in your model. Default: <code>false</code>.</li> <li>device (string) - Device to store Tensor. Either \"gpu\" or \"cpu\". If your device has a gpu, large models will train faster on it.</li> </ul> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.rand([3,2], false, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[0.011, 0.122],\n// [-0.03, 0.105]\n// [-0.90,-0.202]]\n</code></pre> <p></p>"},{"location":"tensor/#tensorbackward","title":"tensor.backward","text":"<p>Performs backpropagation from this tensor backwards. It fills the gradients of every tensor that led to this one with gradients relative to this tensor.</p> <p>Note: This only happens to tensors that have <code>requires_grad</code> set to true.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; let b = torch.randn([2,4], false, 'gpu');\n&gt;&gt;&gt; let c = torch.matmul(a,b);\n&gt;&gt;&gt; c.backward();\n&gt;&gt;&gt; console.log(a.grad);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n</code></pre> <p></p>"},{"location":"tensor/#tensorzero_grad","title":"tensor.zero_grad","text":"<p>Clears the gradients stored in this tensor. Sets the gadients to zero. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; let b = torch.randn([2,4], false, 'gpu');\n&gt;&gt;&gt; let c = torch.matmul(a,b);\n&gt;&gt;&gt; c.backward();\n&gt;&gt;&gt; console.log(a.grad);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n&gt;&gt;&gt; a.zero_grad();\n&gt;&gt;&gt; console.log(a.grad);\n//[[0, 0],\n// [0, 0]\n// [0, 0]]\n</code></pre> <p></p>"},{"location":"tensor/#tensorzero_grad_graph","title":"tensor.zero_grad_graph","text":"<p>Clears the gradients stored in this tensor, setting the gadients to zero, and does the same for every tensor that led to this one. This is used after the gradients have been used to update the parameters of a model, to set the model up for the next iteration.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; let b = torch.randn([2,4], true, 'gpu');\n&gt;&gt;&gt; let c = torch.matmul(a,b);\n&gt;&gt;&gt; c.backward();\n&gt;&gt;&gt; console.log(a.grad);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n&gt;&gt;&gt; c.zero_grad_graph(); // Clears gradients of c, b, and a.\n&gt;&gt;&gt; console.log(a.grad);\n//[[0, 0],\n// [0, 0]\n// [0, 0]]\n</code></pre> <p></p>"},{"location":"tensor/#tensortolist","title":"tensor.tolist","text":"<p>Returns an Array with the tensor's data.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; let aArray = a.tolist();\n&gt;&gt;&gt; console.log(aArray);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n</code></pre> <p></p>"},{"location":"tensor/#tensordata","title":"tensor.data","text":"<p>Returns the tensor's data as a javascript Array.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; console.log(a.data);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n</code></pre> <p></p>"},{"location":"tensor/#tensorlength","title":"tensor.length","text":"<p>Returns the tensor's length (size of first dimension).</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; console.log(a.length);\n// 3\n</code></pre> <p></p>"},{"location":"tensor/#tensorndims","title":"tensor.ndims","text":"<p>Returns the number of dimensions in the Tensor.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2,1,4], true, 'gpu');\n&gt;&gt;&gt; console.log(a.ndims);\n// 4\n</code></pre> <p></p>"},{"location":"tensor/#tensorgrad","title":"tensor.grad","text":"<p>Returns the gradients currently stored in the Tensor.</p> <p>Example</p> <pre><code>&gt;&gt;&gt; let a = torch.randn([3,2], true, 'gpu');\n&gt;&gt;&gt; let b = torch.randn([2,4], true, 'gpu');\n&gt;&gt;&gt; let c = torch.matmul(a,b);\n&gt;&gt;&gt; c.backward();\n&gt;&gt;&gt; console.log(a.grad);\n//[[0.001, -0.99],\n// [-0.13, 0.333]\n// [-0.91,-0.044]]\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section contains ready-to-use examples of JS-PyTorch in action, with increasing complexity and explainations along the way.</p>"},{"location":"tutorials/#gradients","title":"Gradients","text":"<p>To use the autograd functionality (get Tensor's gradients), first create your input tensor, and your parameter tensors. We want to see the gradients of the parameter tensors relative to the output, so we set <code>requires_grad=true</code> on them.</p> <pre><code>const { torch } = require(\"js-pytorch\");\n\n// Instantiate Input Tensor:\nlet x = torch.randn([8, 4, 5], false);\n\n// Instantiate Parameter Tensors:\nlet w = torch.randn([8, 5, 4], true);\nlet b = torch.tensor([0.2, 0.5, 0.1, 0.0], true);\n</code></pre> <ul> <li>The parameter tensor <code>w</code> will be multiplied by the input tensor <code>x</code>.</li> <li>The parameter tensor <code>b</code> will be added to the input tensor <code>x</code>.</li> </ul> <pre><code>// Make calculations:\nlet y = torch.matmul(x, w);\ny = torch.add(out, b);\n</code></pre> <p>Now, compute the gradients of <code>w</code> and <code>b</code> using <code>tensor.backward</code> on the output tensor <code>y</code>.</p> <pre><code>// Compute gradients on whole graph:\ny.backward();\n\n// Get gradients from specific Tensors:\nconsole.log(w.grad);\nconsole.log(b.grad);\n</code></pre> <p>To access the gradients of a tensor after calling <code>tensor.backward</code> on the output, use the syntax <code>tensor.grad</code>.</p>"},{"location":"tutorials/#neural-network","title":"Neural Network","text":"<p>To train a neural network from scratch, first import the <code>torch</code>, <code>torch.nn</code> and <code>torch.optim</code> modules.</p> <pre><code>const torch = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\n</code></pre> <p>Now, create the Neural Network class. This class extends the <code>nn.Module</code> object. In the constructor, add the layers that make up your model, adding them as attributes of your Neural Network class. In this case, there is:</p> <ul> <li>A Linear layer, stored in <code>this.w1</code>.</li> <li>A ReLU activation function, stored in <code>this.relu1</code>.</li> <li>A Linear layer, stored in <code>this.w2</code>.</li> <li>A ReLU activation function, stored in  <code>this.relu2</code>.</li> <li>A Linear layer, stored in <code>this.w3</code>.</li> </ul> <p>The size of these layers depends on three parameters, passed into the constructor: </p> <ul> <li>input_size, defines the size of the last dimension of the input tensor.</li> <li>hidden_size, defines the size of each hidden layer in the model.</li> <li>out_size defines the size of the last dimension of the output tensor. This is the same as the number of classes of the output.</li> </ul> <p>Note: The input_size must be the input size of the first layer, and out_size must be the output size of the last layer.</p> <p>After the constructor, create a forward method, where an input is passed through each layer in the model. To pass the input through a layer, use:</p> <pre><code>this.layer.forward(input);\n</code></pre> <p>The final class is as follows:</p> <pre><code>\n// Implement Module class:\nclass NeuralNet extends nn.Module {\n  constructor(in_size, hidden_size, out_size) {\n    super();\n    // Instantiate Neural Network's Layers:\n    this.w1 = new nn.Linear(in_size, hidden_size);\n    this.relu1 = new nn.ReLU();\n    this.w2 = new nn.Linear(hidden_size, hidden_size);\n    this.relu2 = new nn.ReLU();\n    this.w3 = new nn.Linear(hidden_size, out_size);\n  };\n\n  forward(x) {\n    let z;\n    z = this.w1.forward(x);\n    z = this.relu1.forward(z);\n    z = this.w2.forward(z);\n    z = this.relu2.forward(z);\n    z = this.w3.forward(z);\n    return z;\n  };\n};\n</code></pre> <p>Note: To add the module to the gpu for faster computation, pass the argument <code>'gpu'</code> to the Linear layers.</p> <p>Now, create an instance of your NeuralNetwork class. Declare the in_size, hidden_size and out_size according to your data, and  fine a batch size:</p> <pre><code>// Instantiate Model:\nlet in_size = 16;\nlet hidden_size = 32;\nlet out_size = 10;\nlet batch_size = 16;\n\nlet model = new NeuralNet(in_size,hidden_size,out_size);\n</code></pre> <p>Instantiate the loss function and optimizer, passing the parameters of the models using the <code>model.parameters()</code> method and the learning rate.</p> <pre><code>// Define loss function and optimizer:\nlet loss_func = new nn.CrossEntropyLoss();\nlet optimizer = new optim.Adam(model.parameters(), 3e-3);\n</code></pre> <p>Import the data, and add it to x (input) and y (target) variables.</p> <p>Note: Here, we are generating a dummy dataset (random input and target).</p> <pre><code>// Instantiate input and output:\nlet x = torch.randn([batch_size, in_size]);\nlet y = torch.randint(0, out_size, [batch_size]);\nlet loss;\n</code></pre> <p>Create a train loop to train your model:</p> <pre><code>// Training Loop:\nfor (let i = 0; i &lt; 256; i++) {\n  let z = model.forward(x);\n\n  // Get loss:\n  loss = loss_func.forward(z, y);\n\n  // Backpropagate the loss using torch.tensor's backward() method:\n  loss.backward();\n\n  // Update the weights:\n  optimizer.step();\n\n  // Reset the gradients to zero after each training step:\n  optimizer.zero_grad();\n\n  // Print current loss:\n  console.log(`Iter: ${i} - Loss: ${loss.data}`);\n}\n</code></pre> Detailing   On each pass through the training loop, the following happens:      - The input is passed through the model:   <pre><code>let z = model.forward(x);\n</code></pre>   - The loss is calculated:   <pre><code>loss = loss_func.forward(z, y);\n</code></pre>   - The gradients are computed:    <pre><code>loss.backward();\n</code></pre>   - The parameters are optimized:   <pre><code>optimizer.step();\n</code></pre>   - The gradients are reset:   <pre><code>optimizer.zero_grad();\n</code></pre>   - The current loss is printed to the console:   <pre><code>console.log(`Iter: ${i} - Loss: ${loss.data}`);\n</code></pre> <p> Now, the entire Neural Network, with:</p> <ul> <li>Class declaration.</li> <li>Hyperparameter Definition.</li> <li>Train Loop. </li> </ul> Full Implementation <pre><code>const torch = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\n\n// Implement Module class:\nclass NeuralNet extends nn.Module {\n  constructor(in_size, hidden_size, out_size) {\n    super();\n    // Instantiate Neural Network's Layers:\n    this.w1 = new nn.Linear(in_size, hidden_size);\n    this.relu1 = new nn.ReLU();\n    this.w2 = new nn.Linear(hidden_size, hidden_size);\n    this.relu2 = new nn.ReLU();\n    this.w3 = new nn.Linear(hidden_size, out_size);\n  };\n\n  forward(x) {\n    let z;\n    z = this.w1.forward(x);\n    z = this.relu1.forward(z);\n    z = this.w2.forward(z);\n    z = this.relu2.forward(z);\n    z = this.w3.forward(z);\n    return z;\n  };\n};\n\n// Instantiate Model:\nlet in_size = 16;\nlet hidden_size = 32;\nlet out_size = 10;\nlet batch_size = 16;\n\nlet model = new NeuralNet(in_size,hidden_size,out_size);\n\n// Define loss function and optimizer:\nlet loss_func = new nn.CrossEntropyLoss();\nlet optimizer = new optim.Adam(model.parameters(), 3e-3);\n\n// Instantiate input and output:\nlet x = torch.randn([batch_size, in_size]);\nlet y = torch.randint(0, out_size, [batch_size]);\nlet loss;\n\n// Training Loop:\nfor (let i = 0; i &lt; 256; i++) {\n  let z = model.forward(x);\n\n  // Get loss:\n  loss = loss_func.forward(z, y);\n\n  // Backpropagate the loss using torch.tensor's backward() method:\n  loss.backward();\n\n  // Update the weights:\n  optimizer.step();\n\n  // Reset the gradients to zero after each training step:\n  optimizer.zero_grad();\n\n  // Print current loss:\n  console.log(`Iter: ${i} - Loss: ${loss.data}`);\n}\n</code></pre> <p></p>"},{"location":"tutorials/#transformer","title":"Transformer","text":"<p>Following the exact same steps as the last tutorial, we can create a Transformer Model.</p> <p>The inputs of the constructor are:</p> <ul> <li>vocab_size, defines the size of the last dimension of the input and output tensor. It is the number of characters or words that compose your vocabulary.</li> <li>hidden_size, defines the size of each hidden layer in the model.</li> <li>n_timesteps number of timesteps computed in parallel by the transformer.</li> <li>dropout_p probability of randomly dropping an activation during training (to improve regularization).</li> <li>device is the device on which the model's calculations will run. Either <code>'cpu'</code> or <code>'gpu'</code>.</li> </ul> Full Implementation <pre><code>const { torch } = require(\"js-pytorch\");\nconst nn = torch.nn;\nconst optim = torch.optim;\nconst device = 'gpu';\n\n// Create Transformer decoder Module:\nclass Transformer extends nn.Module {\n  constructor(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device) {\n    super();\n    // Instantiate Transformer's Layers:\n    this.embed = new nn.Embedding(vocab_size, hidden_size);\n    this.pos_embed = new nn.PositionalEmbedding(n_timesteps, hidden_size);\n    this.b1 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device);\n    this.b2 = new nn.Block(hidden_size, hidden_size, n_heads, n_timesteps, dropout_p, device);\n    this.ln = new nn.LayerNorm(hidden_size);\n    this.linear = new nn.Linear(hidden_size, vocab_size, device);\n  }\n\n  forward(x) {\n    let z;\n    z = torch.add(this.embed.forward(x), this.pos_embed.forward(x));\n    z = this.b1.forward(z);\n    z = this.b2.forward(z);\n    z = this.ln.forward(z);\n    z = this.linear.forward(z);\n    return z;\n  }\n}\n\n// Define training hyperparameters:\nconst vocab_size = 52;\nconst hidden_size = 32;\nconst n_timesteps = 16;\nconst n_heads = 4;\nconst dropout_p = 0;\nconst batch_size = 8;\n\n// Instantiate your custom nn.Module:\nconst model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p, device);\n\n// Define loss function and optimizer:\nconst loss_func = new nn.CrossEntropyLoss();\nconst optimizer = new optim.Adam(model.parameters(), (lr = 5e-3), (reg = 0));\n\n// Instantiate sample input and output:\nlet x = torch.randint(0, vocab_size, [batch_size, n_timesteps, 1]);\nlet y = torch.randint(0, vocab_size, [batch_size, n_timesteps]);\nlet loss;\n\n// Training Loop:\nfor (let i = 0; i &lt; 256; i++) {\n  // Forward pass through the Transformer:\n  let z = model.forward(x);\n\n  // Get loss:\n  loss = loss_func.forward(z, y);\n\n  // Backpropagate the loss using torch.tensor's backward() method:\n  loss.backward();\n\n  // Update the weights:\n  optimizer.step();\n\n  // Reset the gradients to zero after each training step:\n  optimizer.zero_grad();\n\n  // Print loss at every iteration:\n  console.log(`Iter ${i} - Loss ${loss.data[0].toFixed(4)}`)\n}\n</code></pre> <p></p>"},{"location":"tutorials/#saving-and-loading-models","title":"Saving and Loading Models","text":"<p>To save a model, first instantiate a class extending <code>nn.Module</code> for your model, as explained in the previous tutorials.</p> <pre><code>// Instantiate your model:\nconst model = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p);\n</code></pre> <p>Then, train your model. When you are finished, or during training (to generate snapshots), save the model to a JSON file using <code>torch.save()</code>:</p> <pre><code>// Save model to JSON file:\ntorch.save(model, 'model.json')\n</code></pre> <p>To load the model, instantiate a placeholder as an empty instance of the same model:</p> <pre><code>// To load, instantiate placeHolder using the original model's architecture:\nconst placeHolder = new Transformer(vocab_size, hidden_size, n_timesteps, n_heads, dropout_p);\n</code></pre> <p>Then, load the weights of the trained model into the placeholder using <code>torch.load()</code>:</p> <pre><code>// Load weights into placeHolder:\nconst newModel = torch.load(placeHolder, 'model.json')\n</code></pre>"},{"location":"tutorials/#testing","title":"Testing","text":"<p>To test a model, just run your test data through the trained model using <code>model.forward()</code>:</p> <pre><code>// Load weights into placeHolder:\nlet z = model.forward(x);\n</code></pre> <p>Then, use a loss function or a custom function to calculate your loss or accuracy in comparaison with the target:</p> <pre><code>let loss = nn.CrossEntropyLoss(z,y);\n</code></pre>"}]}